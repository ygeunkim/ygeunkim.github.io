<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Young-geun Kim</title>
    <link>https://ygeunkim.github.io/authors/admin/</link>
      <atom:link href="https://ygeunkim.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Young-geun Kim</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 05 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ygeunkim.github.io/img/icon-192.png</url>
      <title>Young-geun Kim</title>
      <link>https://ygeunkim.github.io/authors/admin/</link>
    </image>
    
    <item>
      <title>Contextual Anomaly Detection by Correlated Probability Distributions using Kullback-Leibler Divergence (Poster)</title>
      <link>https://ygeunkim.github.io/publication/kl_poster/kl_poster/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://ygeunkim.github.io/publication/kl_poster/kl_poster/</guid>
      <description>&lt;!-- Jinwoo Cho, Shahroz Tariq, Sangyup Lee, Young Geun Kim, Jeong-Han Yun, Jonguk Kim, Hyoung Chun Kim and Simon S. Woo, *Contextual Anomaly Detection by Correlated Probability Distributions using Kullback-Leibler Divergence, 5th Workshop on Mining and Learning from Time Series*, held in conjunction with KDD&#39;19 Aug 5, 2019 - Anchorage, Alaska, USA --&gt;

&lt;!-- *** --&gt;

&lt;!-- **Abstract** We investigate anomaly detection in Cyber-Physical System (CPS), where often anomalies are attacks to CPS to disrupt the operations of critical infrastructures. We use secure water treatment (SWaT) systems dataset, where normal and attack states are simulated at the water tanks. Among different types of anomalies, we focus on detecting contextual anomaly, which can be difficult to detect from Out-Of-Limit threshold method. Recent research shows the promising result in detecting anomalies from analyzing error distributions from the machine learning classifier. In a similar way, we statistically analyze prediction error patterns from RNN and MDN classifiers to detect anomalies. First, we generate anomaly scores with Local Outlier Factor (LOF) and remove point anomalies. With the fixed window size, empirical probability distribution is estimated, and we apply the sliding window to measure the difference of probability distributions between the other windows. To measure the difference efficiently between anomalies and normal data, we use Kullback-Leibler divergence. Our preliminary result shows that we can effectively detect contextual anomalies compared with Nearest Neighbor Distance (NND) approach. --&gt;
</description>
    </item>
    
  </channel>
</rss>
